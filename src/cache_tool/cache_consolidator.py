import pandas as pd
from pathlib import Path

from .cache_utils import get_sorted_cache_files, get_file_info
from .cache_file_io import write_to_cache, read_cache_file
from .cache_data_processor import merge_with_deduplication


def consolidate_cache(
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = "parquet",
) -> None:
    """
    æ•´ç†ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚

    æŸ¥æ‰¾è¿ç»­çš„ã€å¤§å°å°äº cache_size çš„ç¼“å­˜æ–‡ä»¶ï¼Œå°†å®ƒä»¬åˆ†ç»„ï¼Œç„¶ååˆå¹¶åé‡æ–°å†™å…¥ã€‚
    """
    if not cache_dir.exists():
        print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ•´ç†ã€‚")
        return

    sorted_cache_files = get_sorted_cache_files(cache_dir, symbol, period, file_type)
    if not sorted_cache_files:
        print("âœ… ç¼“å­˜ç›®å½•ä¸­æ²¡æœ‰éœ€è¦æ•´ç†çš„æœ‰æ•ˆæ–‡ä»¶ã€‚")
        return

    print(f"\n--- å¼€å§‹å°è¯•æ•´ç† {symbol} {period} çš„ç¼“å­˜æ–‡ä»¶ ---")

    # ä½¿ç”¨äºŒç»´åˆ—è¡¨æ¥æ”¶é›†è¿ç»­çš„æ–‡ä»¶å—
    files_to_merge = []

    current_group = []
    for i in range(len(sorted_cache_files)):
        current_file = sorted_cache_files[i]
        current_info = get_file_info(current_file.name)
        if not current_info:
            continue

        # åªæœ‰å½“æ–‡ä»¶å¤§å°å°äº cache_size æ—¶æ‰è€ƒè™‘å°†å…¶åŠ å…¥å¾…åˆå¹¶é˜Ÿåˆ—
        if current_info["count"] < cache_size:
            # æ£€æŸ¥æ˜¯å¦ä¸å‰ä¸€ä¸ªæ–‡ä»¶è¿ç»­
            is_continuous = False
            if current_group:
                last_file_info = get_file_info(current_group[-1].name)
                # æ£€æŸ¥å½“å‰æ–‡ä»¶çš„å¼€å§‹æ—¶é—´æ˜¯å¦ä¸å‰ä¸€ä¸ªæ–‡ä»¶çš„ç»“æŸæ—¶é—´ç›¸ç­‰
                if (
                    last_file_info
                    and current_info["start_time"] == last_file_info["end_time"]
                ):
                    is_continuous = True

            if not current_group or is_continuous:
                current_group.append(current_file)
                print(f"âœ… å°†æ–‡ä»¶ {current_file.name} æ·»åŠ åˆ°å½“å‰è¿ç»­ç»„ã€‚")
            else:
                # é‡åˆ°ä¸è¿ç»­çš„æ–‡ä»¶ï¼Œä¿å­˜å½“å‰ç»„å¹¶å¼€å§‹æ–°çš„ç»„
                files_to_merge.append(current_group)
                current_group = [current_file]
                print(f"âš ï¸ é‡åˆ°ä¸è¿ç»­ï¼Œæ–°å¼€ä¸€ç»„ï¼Œæ·»åŠ æ–‡ä»¶ {current_file.name}ã€‚")
        else:
            # é‡åˆ°å¤§äºæˆ–ç­‰äº cache_size çš„æ–‡ä»¶ï¼Œç»“æŸå½“å‰ç»„
            if current_group:
                files_to_merge.append(current_group)
                current_group = []

    # å¾ªç¯ç»“æŸæ—¶ï¼Œä¿å­˜æœ€åä¸€ä¸ªç»„
    if current_group:
        files_to_merge.append(current_group)

    print("\n--- å¼€å§‹åˆå¹¶å’Œé‡æ–°å†™å…¥ç¼“å­˜æ–‡ä»¶ ---")
    merged_count = 0

    for group in files_to_merge:
        # åªæœ‰å½“ä¸€ä¸ªç»„åŒ…å«å¤šäºä¸€ä¸ªæ–‡ä»¶æ—¶æ‰è¿›è¡Œåˆå¹¶
        if len(group) > 1:
            merged_count += 1
            print(f"--- æ­£åœ¨å¤„ç†ç¬¬ {merged_count} ä¸ªå¾…åˆå¹¶æ–‡ä»¶å— ---")

            merged_data = pd.DataFrame()

            # 1. åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ–‡ä»¶
            for f in group:
                try:
                    data_to_merge = read_cache_file(f, file_type)
                    merged_data = merge_with_deduplication(merged_data, data_to_merge)
                    print(f"ğŸ“¦ å·²åŠ è½½å¹¶åˆå¹¶æ–‡ä»¶: {f.name}")
                except Exception as e:
                    print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {f.name}: {e}")

            # 2. åˆ é™¤æ—§æ–‡ä»¶
            for f in group:
                if f.exists():
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶: {f.name}")
                    f.unlink()

            # 3. å†™å…¥æ–°åˆå¹¶çš„æ•°æ®
            write_to_cache(
                symbol, period, merged_data, cache_dir, cache_size, file_type
            )

    if merged_count == 0:
        print("âœ… æ²¡æœ‰éœ€è¦åˆå¹¶çš„æ–‡ä»¶å—ï¼Œç¼“å­˜å·²æ˜¯æœ€ä¼˜çŠ¶æ€ã€‚")

    print("--- ç¼“å­˜æ•´ç†å®Œæˆ ---")
