import pandas as pd
from pathlib import Path

from .cache_utils import get_sorted_cache_files, get_file_info
from .cache_file_io import write_to_cache, read_cache_file
from .cache_data_processor import merge_with_deduplication


def consolidate_cache(
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = "parquet",
) -> None:
    """
    æ•´ç†ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚

    æŸ¥æ‰¾è¿ç»­çš„ã€å¤§å°å°äº cache_size çš„ç¼“å­˜æ–‡ä»¶ï¼Œå°†å®ƒä»¬åˆ†ç»„ï¼Œç„¶ååˆå¹¶åé‡æ–°å†™å…¥ã€‚
    """
    if not cache_dir.exists():
        print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ•´ç†ã€‚")
        return

    sorted_cache_files = get_sorted_cache_files(cache_dir, symbol, period, file_type)
    if not sorted_cache_files:
        print("âœ… ç¼“å­˜ç›®å½•ä¸­æ²¡æœ‰éœ€è¦æ•´ç†çš„æœ‰æ•ˆæ–‡ä»¶ã€‚")
        return

    print(f"\n--- å¼€å§‹å°è¯•æ•´ç† {symbol} {period} çš„ç¼“å­˜æ–‡ä»¶ ---")

    # ä½¿ç”¨äºŒç»´åˆ—è¡¨æ¥æ”¶é›†è¿ç»­çš„æ–‡ä»¶å—
    files_to_merge = []

    current_group = []
    for i in range(len(sorted_cache_files)):
        current_file = sorted_cache_files[i]
        current_info = get_file_info(current_file.name)
        if not current_info:
            continue

        # åªæœ‰å½“æ–‡ä»¶å¤§å°å°äº cache_size æ—¶æ‰è€ƒè™‘å°†å…¶åŠ å…¥å¾…åˆå¹¶é˜Ÿåˆ—
        if current_info["count"] < cache_size:
            # æ£€æŸ¥æ˜¯å¦ä¸å‰ä¸€ä¸ªæ–‡ä»¶è¿ç»­
            is_continuous = False
            if current_group:
                last_file_info = get_file_info(current_group[-1].name)
                # æ£€æŸ¥å½“å‰æ–‡ä»¶çš„å¼€å§‹æ—¶é—´æ˜¯å¦ä¸å‰ä¸€ä¸ªæ–‡ä»¶çš„ç»“æŸæ—¶é—´ç›¸ç­‰
                if (
                    last_file_info
                    and current_info["start_time"] == last_file_info["end_time"]
                ):
                    is_continuous = True

            if not current_group or is_continuous:
                current_group.append(current_file)
                print(f"âœ… å°†æ–‡ä»¶ {current_file.name} æ·»åŠ åˆ°å½“å‰è¿ç»­ç»„ã€‚")
            else:
                # é‡åˆ°ä¸è¿ç»­çš„æ–‡ä»¶ï¼Œä¿å­˜å½“å‰ç»„å¹¶å¼€å§‹æ–°çš„ç»„
                files_to_merge.append(current_group)
                current_group = [current_file]
                print(f"âš ï¸ é‡åˆ°ä¸è¿ç»­ï¼Œæ–°å¼€ä¸€ç»„ï¼Œæ·»åŠ æ–‡ä»¶ {current_file.name}ã€‚")
        else:
            # é‡åˆ°å¤§äºæˆ–ç­‰äº cache_size çš„æ–‡ä»¶ï¼Œç»“æŸå½“å‰ç»„
            if current_group:
                files_to_merge.append(current_group)
                current_group = []

    # å¾ªç¯ç»“æŸæ—¶ï¼Œä¿å­˜æœ€åä¸€ä¸ªç»„
    if current_group:
        files_to_merge.append(current_group)

    print("\n--- å¼€å§‹åˆå¹¶å’Œé‡æ–°å†™å…¥ç¼“å­˜æ–‡ä»¶ ---")
    merged_count = 0

    for group in files_to_merge:
        # åªæœ‰å½“ä¸€ä¸ªç»„åŒ…å«å¤šäºä¸€ä¸ªæ–‡ä»¶æ—¶æ‰è¿›è¡Œåˆå¹¶
        if len(group) > 1:
            merged_count += 1
            print(f"--- æ­£åœ¨å¤„ç†ç¬¬ {merged_count} ä¸ªå¾…åˆå¹¶æ–‡ä»¶å— ---")

            merged_data = pd.DataFrame()

            # 1. åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ–‡ä»¶
            for f in group:
                try:
                    data_to_merge = read_cache_file(f, file_type)
                    merged_data = merge_with_deduplication(merged_data, data_to_merge)
                    print(f"ğŸ“¦ å·²åŠ è½½å¹¶åˆå¹¶æ–‡ä»¶: {f.name}")
                except Exception as e:
                    print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {f.name}: {e}")

            # 2. åˆ é™¤æ—§æ–‡ä»¶
            for f in group:
                if f.exists():
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶: {f.name}")
                    f.unlink()

            # 3. å†™å…¥æ–°åˆå¹¶çš„æ•°æ®
            write_to_cache(
                symbol, period, merged_data, cache_dir, cache_size, file_type
            )

    if merged_count == 0:
        print("âœ… æ²¡æœ‰éœ€è¦åˆå¹¶çš„æ–‡ä»¶å—ï¼Œç¼“å­˜å·²æ˜¯æœ€ä¼˜çŠ¶æ€ã€‚")

    print("--- ç¼“å­˜æ•´ç†å®Œæˆ ---")


def check_for_overlaps(
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = "parquet",
) -> None:
    """
    æ£€æŸ¥ç¼“å­˜ç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æ–‡ä»¶æ—¶é—´é‡å ï¼Œå¹¶å¤„ç†é‡å éƒ¨åˆ†ã€‚

    å¦‚æœå‘ç°é‡å ï¼Œå°†ä¿ç•™æœ€æ–°æ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œå¹¶åˆ é™¤æ—§æ–‡ä»¶ä¸­çš„é‡å éƒ¨åˆ†ã€‚
    """
    print(f"\n--- å¼€å§‹æ£€æŸ¥ {symbol} {period} çš„ç¼“å­˜æ–‡ä»¶é‡å æƒ…å†µ ---")

    sorted_files = get_sorted_cache_files(cache_dir, symbol, period, file_type)
    if len(sorted_files) < 2:
        print("âœ… æ–‡ä»¶æ•°é‡ä¸è¶³ï¼Œæ— éœ€æ£€æŸ¥é‡å ã€‚")
        return

    # éå†æ‰€æœ‰æ–‡ä»¶ï¼Œå°†ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ç»“æŸæ—¶é—´ä¸åç»­æ–‡ä»¶çš„å¼€å§‹æ—¶é—´è¿›è¡Œæ¯”è¾ƒ
    for i in range(len(sorted_files) - 1):
        file_a = sorted_files[i]
        file_b = sorted_files[i + 1]

        info_a = get_file_info(file_a.name)
        info_b = get_file_info(file_b.name)

        if not info_a or not info_b:
            continue

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å 
        # å¦‚æœ A çš„ç»“æŸæ—¶é—´ > B çš„å¼€å§‹æ—¶é—´ï¼Œè¯´æ˜æœ‰é‡å 
        if info_a["end_time"] > info_b["start_time"]:
            print(f"âš ï¸ å‘ç°é‡å ï¼æ–‡ä»¶ {file_a.name} å’Œ {file_b.name} å­˜åœ¨æ—¶é—´é‡å ã€‚")
            print(f"   > æ–‡ä»¶Aæ—¶é—´èŒƒå›´: {info_a['start_time']} - {info_a['end_time']}")
            print(f"   > æ–‡ä»¶Bæ—¶é—´èŒƒå›´: {info_b['start_time']} - {info_b['end_time']}")

            # åŠ è½½æ–‡ä»¶Açš„æ•°æ®
            df_a = read_cache_file(file_a, file_type)
            if df_a.empty:
                print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_a.name}ï¼Œè·³è¿‡å¤„ç†ã€‚")
                continue

            # ç¡®å®šé‡å æ—¶é—´æ®µçš„å¼€å§‹ç‚¹ (å–æ–‡ä»¶Bçš„å¼€å§‹æ—¶é—´ï¼Œè¿™æ˜¯æ–°æ•°æ®çš„èµ·ç‚¹)
            overlap_start_time = info_b["start_time"]

            # ä»æ–‡ä»¶Aä¸­åˆ é™¤ä¸æ–‡ä»¶Bé‡å çš„éƒ¨åˆ†
            original_len_a = len(df_a)
            # ä¿ç•™Aä¸­æ—¶é—´æˆ³ <= æ–‡ä»¶Bå¼€å§‹æ—¶é—´çš„æ•°æ®
            df_a_new = df_a[df_a["time"] <= overlap_start_time]

            if len(df_a_new) < original_len_a:
                print(
                    f"ğŸ”„ æ­£åœ¨ç§»é™¤æ–‡ä»¶ {file_a.name} ä¸­çš„ {original_len_a - len(df_a_new)} æ¡é‡å æ•°æ®ã€‚"
                )

                # å¦‚æœAä¸­æ‰€æœ‰æ•°æ®éƒ½é‡å ï¼Œåˆ™åˆ é™¤æ–‡ä»¶A
                if df_a_new.empty:
                    print(f"ğŸ—‘ï¸ æ–‡ä»¶ {file_a.name} å·²è¢«å®Œå…¨è¦†ç›–ï¼Œåˆ é™¤æ—§æ–‡ä»¶ã€‚")
                    file_a.unlink()
                else:
                    # åˆ é™¤æ—§æ–‡ä»¶A
                    file_a.unlink()
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ–‡ä»¶: {file_a.name}")

                    # ä½¿ç”¨å†™ç¼“å­˜å‡½æ•°ï¼Œå®ƒå¯ä»¥å¤„ç†æ–‡ä»¶å‘½åå’Œè·¯å¾„
                    write_to_cache(
                        symbol, period, df_a_new, cache_dir, cache_size, file_type
                    )

    print("\n--- é‡å æ£€æŸ¥å’Œæ¸…ç†å®Œæˆ ---")
