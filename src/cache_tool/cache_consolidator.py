import polars as pl
from pathlib import Path

from .cache_utils import (
    get_sorted_cache_files,
    get_file_info,
    group_continuous_files,
)
from .cache_file_io import write_to_cache, read_cache_file
from .cache_data_processor import merge_with_deduplication
from .cache_utils import (
    find_max_diff_sequence,
    find_cache_size_sequences,
)


def _process_stream(
    files_to_process: list[Path],
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = "parquet",
    reverse: bool = False,
    start_time: int | None = None,  # è°ƒè¯•ç”¨
) -> None:
    """
    é‡‡ç”¨â€œæµå¼å¤„ç†â€çš„é€»è¾‘æ¥åˆå¹¶å°æ–‡ä»¶ã€‚

    æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
    1. ä½¿ç”¨ä¸€ä¸ªå•ä¸€çš„å†…å­˜ç¼“å†²åŒºï¼ˆcurrent_dfï¼‰æ¥ç´¯ç§¯æ•°æ®ã€‚
    2. å¦‚æœ `reverse` ä¸º `True`ï¼Œåˆ™é€†åºè¯»å–æ–‡ä»¶ï¼Œä»¥å®ç°å‘å‰åˆå¹¶ã€‚
    3. æ¯è¯»å–ä¸€ä¸ªæ–‡ä»¶ï¼Œå°±å°†å®ƒçš„æ•°æ®ä¸ç¼“å†²åŒº `current_df` åˆå¹¶å¹¶å»é‡ã€‚
    4. å½“ç¼“å†²åŒºçš„æ•°æ®é‡è¾¾åˆ°æˆ–è¶…è¿‡ `cache_size` æ—¶ï¼Œç«‹å³å°†ç¼“å†²åŒºçš„æ•°æ®å†™å…¥æ–°æ–‡ä»¶ã€‚
    5. `write_to_cache` å‡½æ•°å†…éƒ¨å¤„ç†åˆ†å—å†™å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ–°æ–‡ä»¶çš„åˆ—è¡¨ã€‚
    6. å‡½æ•°ä¼šè¯»å–æ–°æ–‡ä»¶åˆ—è¡¨ä¸­çš„æœ€åä¸€ä¸ªæ–‡ä»¶ï¼ˆä»£è¡¨å‰©ä½™æ•°æ®ï¼‰ï¼Œå°†å…¶åŠ è½½åˆ°å†…å­˜ä½œä¸ºæ–°çš„ `current_df`ï¼Œå¹¶ä»ç£ç›˜ä¸­åˆ é™¤è¯¥æ–‡ä»¶ã€‚
    7. æ¯å¤„ç†å®Œä¸€ä¸ªæ–‡ä»¶ï¼Œå°±ç«‹å³å°†å…¶ä»ç£ç›˜ä¸­åˆ é™¤ï¼Œé¿å…ç­‰å¾…ã€‚
    """
    current_df = pl.DataFrame()

    # æ ¹æ® reverse å‚æ•°å†³å®šæ–‡ä»¶éå†é¡ºåº
    if reverse:
        files_to_process = files_to_process[::-1]

    for file_path in files_to_process:
        df_to_add = read_cache_file(file_path, file_type)

        if df_to_add.is_empty():
            file_path.unlink()  # æ–‡ä»¶ä¸ºç©ºï¼Œç›´æ¥åˆ é™¤
            continue

        # æ ¹æ®å¤„ç†æ–¹å‘è°ƒæ•´åˆå¹¶é¡ºåºï¼Œç¡®ä¿æ—¶é—´åºåˆ—çš„æ­£ç¡®æ€§
        if reverse:
            # åå‘å¤„ç†æ—¶ï¼Œå°†æ–°æ•°æ®æ”¾åœ¨å‰é¢
            current_df = merge_with_deduplication(df_to_add, current_df)
        else:
            # æ­£å¸¸å¤„ç†æ—¶ï¼Œå°†æ–°æ•°æ®æ”¾åœ¨åé¢
            current_df = merge_with_deduplication(current_df, df_to_add)

        # å¤„ç†å®Œå½“å‰æ–‡ä»¶åï¼Œç«‹å³åˆ é™¤
        file_path.unlink()

        # å½“ç´¯ç§¯æ•°æ®è¾¾åˆ°æˆ–è¶…è¿‡ cache_size æ—¶ï¼Œè¿›è¡Œå¤„ç†å’Œå†™å…¥
        if len(current_df) >= cache_size:
            written_files = write_to_cache(
                symbol,
                period,
                current_df,
                cache_dir,
                cache_size,
                file_type,
                reverse=reverse,
            )

            current_df = pl.DataFrame()
            if written_files:
                last_file = written_files[0] if reverse else written_files[-1]
                last_file_info = get_file_info(last_file.name)
                if last_file_info and last_file_info.get("count", 0) < cache_size:
                    current_df = read_cache_file(last_file, file_type)
                    last_file.unlink()

    # å¾ªç¯ç»“æŸåï¼Œå¤„ç†ç¼“å†²åŒºä¸­å‰©ä½™çš„æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    if not current_df.is_empty():
        write_to_cache(
            symbol,
            period,
            current_df,
            cache_dir,
            cache_size,
            file_type,
            reverse=reverse,
        )


def _get_files_to_process(
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = "parquet",
):
    """
    ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæŒ‰ç»„ç”Ÿæˆéœ€è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨ã€‚
    å®ƒä¼šæ‰¾åˆ°è¿ç»­çš„æ–‡ä»¶åºåˆ—ï¼Œå¹¶å°†å…¶åˆ’åˆ†ä¸ºå‘å‰åˆå¹¶å’Œå‘ååˆå¹¶ä¸¤éƒ¨åˆ†ã€‚

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. è·å–æ‰€æœ‰ç¼“å­˜æ–‡ä»¶å¹¶æŒ‰æ—¶é—´åˆ†ç»„ã€‚
    2. åœ¨æ¯ä¸ªæ–‡ä»¶ç»„ä¸­ï¼Œæ‰¾åˆ°ç”±â€œæ»¡æ–‡ä»¶â€ï¼ˆæ–‡ä»¶è¡Œæ•° == cache_sizeï¼‰ç»„æˆçš„æœ€é•¿è¿ç»­åºåˆ—ã€‚
    3. ä»¥è¿™ä¸ªæœ€é•¿åºåˆ—ä¸ºâ€œæ ¸å¿ƒâ€ï¼Œå°†åºåˆ—ä¹‹å‰çš„æ–‡ä»¶åˆ†é…ç»™â€œå‘å‰åˆå¹¶â€å¤„ç†ï¼Œå°†åºåˆ—ä¹‹åçš„æ–‡ä»¶åˆ†é…ç»™â€œå‘ååˆå¹¶â€å¤„ç†ã€‚
    4. å¦‚æœæ²¡æœ‰æ»¡æ–‡ä»¶åºåˆ—ï¼Œåˆ™æ‰€æœ‰æ–‡ä»¶éƒ½åˆ†é…ç»™â€œå‘ååˆå¹¶â€å¤„ç†ã€‚

    Yields:
        tuple[list[Path], list[Path]]: ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸¤ä¸ªåˆ—è¡¨ï¼š
        ç¬¬ä¸€ä¸ªæ˜¯éœ€è¦å‘å‰åˆå¹¶çš„æ–‡ä»¶åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªæ˜¯éœ€è¦å‘ååˆå¹¶çš„æ–‡ä»¶åˆ—è¡¨ã€‚
    """
    # 1. è·å–å¹¶åˆ†ç»„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
    sorted_cache_files = get_sorted_cache_files(cache_dir, symbol, period, file_type)
    if not sorted_cache_files:
        # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
        return

    sorted_cache_files_2d = group_continuous_files(sorted_cache_files)

    # 2. éå†æ¯ä¸ªæ–‡ä»¶ç»„å¹¶å¤„ç†
    for all_files in sorted_cache_files_2d:
        cache_size_sequences = find_cache_size_sequences(all_files, cache_size)

        # 3. æ ¹æ®æ˜¯å¦æœ‰æ»¡æ–‡ä»¶åºåˆ—æ¥åˆ’åˆ†æ–‡ä»¶
        if not cache_size_sequences:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ»¡æ–‡ä»¶åºåˆ—ï¼Œæ‰€æœ‰æ–‡ä»¶éƒ½åº”è¯¥å‘ååˆå¹¶
            files_to_process_forward = []
            files_to_process_backward = all_files
        else:
            # æ‰¾åˆ°ç”±æ»¡æ–‡ä»¶ç»„æˆçš„æœ€é•¿è¿ç»­åºåˆ—çš„èµ·å§‹å’Œç»“æŸç´¢å¼•
            max_seq_info = find_max_diff_sequence(cache_size_sequences)

            if max_seq_info is None:
                files_to_process_forward = []
                files_to_process_backward = all_files
            else:
                _, max_seq_start, max_seq_end = max_seq_info

                # å°†æœ€é•¿åºåˆ—ä¹‹å‰çš„æ–‡ä»¶ä½œä¸ºå‘å‰åˆå¹¶çš„æ•°ç»„
                files_to_process_forward = all_files[:max_seq_start]
                # å°†æœ€é•¿åºåˆ—ä¹‹åçš„æ–‡ä»¶ä½œä¸ºå‘ååˆå¹¶çš„æ•°ç»„
                files_to_process_backward = all_files[max_seq_end:]

        # 4. ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨ï¼Œä¾›åç»­å¤„ç†
        yield files_to_process_forward, files_to_process_backward


def consolidate_cache(
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = "parquet",
    start_time: int | None = None,  # è°ƒè¯•ç”¨
) -> None:
    """
    æ•´ç†ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚

    è¯¥å‡½æ•°ä½¿ç”¨ç”Ÿæˆå™¨æ¥è¿­ä»£éœ€è¦å¤„ç†çš„æ–‡ä»¶ç»„ï¼Œä»è€Œå®ç°å†…å­˜é«˜æ•ˆçš„å¤„ç†ã€‚
    """
    for files_to_process_forward, files_to_process_backward in _get_files_to_process(
        cache_dir, cache_size, symbol, period, file_type
    ):
        # å¯¹å‘å‰éƒ¨åˆ†è¿›è¡Œæµå¼å¤„ç†ï¼Œå¹¶è®¾ç½® reverse=True æ¥å®ç°å‘å‰åˆå¹¶
        _process_stream(
            files_to_process_forward,
            cache_dir,
            cache_size,
            symbol,
            period,
            file_type,
            reverse=True,
            start_time=start_time,  # è°ƒè¯•ç”¨
        )

        # å¯¹å‘åéƒ¨åˆ†è¿›è¡Œæµå¼å¤„ç†
        _process_stream(
            files_to_process_backward,
            cache_dir,
            cache_size,
            symbol,
            period,
            file_type,
            reverse=False,
            start_time=start_time,  # è°ƒè¯•ç”¨
        )


def check_for_overlaps(
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = "parquet",
) -> None:
    """
    æ£€æŸ¥ç¼“å­˜ç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æ–‡ä»¶æ—¶é—´é‡å ï¼Œå¹¶å¤„ç†é‡å éƒ¨åˆ†ã€‚

    å¦‚æœå‘ç°é‡å ï¼Œå°†ä¿ç•™æœ€æ–°æ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œå¹¶åˆ é™¤æ—§æ–‡ä»¶ä¸­çš„é‡å éƒ¨åˆ†ã€‚
    """
    print(f"\n--- å¼€å§‹æ£€æŸ¥ {symbol} {period} çš„ç¼“å­˜æ–‡ä»¶é‡å æƒ…å†µ ---")

    sorted_files = get_sorted_cache_files(cache_dir, symbol, period, file_type)
    if len(sorted_files) < 2:
        print("âœ… æ–‡ä»¶æ•°é‡ä¸è¶³ï¼Œæ— éœ€æ£€æŸ¥é‡å ã€‚")
        return

    # éå†æ‰€æœ‰æ–‡ä»¶ï¼Œå°†ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ç»“æŸæ—¶é—´ä¸åç»­æ–‡ä»¶çš„å¼€å§‹æ—¶é—´è¿›è¡Œæ¯”è¾ƒ
    for i in range(len(sorted_files) - 1):
        file_a = sorted_files[i]
        file_b = sorted_files[i + 1]

        info_a = get_file_info(file_a.name)
        info_b = get_file_info(file_b.name)

        if not info_a or not info_b:
            continue

        # æ£€æŸ¥æ–‡ä»¶Bæ˜¯å¦å®Œå…¨åŒ…å«æ–‡ä»¶A
        if (
            info_b["start_time"] <= info_a["start_time"]
            and info_b["end_time"] >= info_a["end_time"]
        ):
            print(f"ğŸ”„ æ–‡ä»¶B {file_b.name} å®Œå…¨åŒ…å«æ—§æ–‡ä»¶A {file_a.name}ã€‚åˆ é™¤æ–‡ä»¶Aã€‚")
            file_a.unlink()
            continue

        # æ£€æŸ¥æ–‡ä»¶Aæ˜¯å¦å®Œå…¨åŒ…å«æ–‡ä»¶B
        if (
            info_a["start_time"] <= info_b["start_time"]
            and info_a["end_time"] >= info_b["end_time"]
        ):
            print(f"ğŸ”„ æ–‡ä»¶A {file_a.name} å®Œå…¨åŒ…å«æ–°æ–‡ä»¶B {file_b.name}ã€‚åˆ é™¤æ–‡ä»¶Bã€‚")
            file_b.unlink()
            continue

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å 
        # å¦‚æœ A çš„ç»“æŸæ—¶é—´ > B çš„å¼€å§‹æ—¶é—´ï¼Œè¯´æ˜æœ‰é‡å 
        if info_a["end_time"] > info_b["start_time"]:
            print(f"âš ï¸ å‘ç°é‡å ï¼æ–‡ä»¶ {file_a.name} å’Œ {file_b.name} å­˜åœ¨æ—¶é—´é‡å ã€‚")
            print(f"   > æ–‡ä»¶Aæ—¶é—´èŒƒå›´: {info_a['start_time']} - {info_a['end_time']}")
            print(f"   > æ–‡ä»¶Bæ—¶é—´èŒƒå›´: {info_b['start_time']} - {info_b['end_time']}")

            # åŠ è½½æ–‡ä»¶Açš„æ•°æ®
            df_a = read_cache_file(file_a, file_type)
            if df_a.is_empty():
                print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_a.name}ï¼Œè·³è¿‡å¤„ç†ã€‚")
                continue

            # ç¡®å®šé‡å æ—¶é—´æ®µçš„å¼€å§‹ç‚¹ (å–æ–‡ä»¶Bçš„å¼€å§‹æ—¶é—´ï¼Œè¿™æ˜¯æ–°æ•°æ®çš„èµ·ç‚¹)
            overlap_start_time = info_b["start_time"]

            # ä»æ–‡ä»¶Aä¸­åˆ é™¤ä¸æ–‡ä»¶Bé‡å çš„éƒ¨åˆ†
            original_len_a = len(df_a)
            # ä¿ç•™Aä¸­æ—¶é—´æˆ³ <= æ–‡ä»¶Bå¼€å§‹æ—¶é—´çš„æ•°æ®
            df_a_new = df_a.filter(pl.col("time") <= overlap_start_time)

            if len(df_a_new) < original_len_a:
                print(
                    f"ğŸ”„ æ­£åœ¨ç§»é™¤æ–‡ä»¶ {file_a.name} ä¸­çš„ {original_len_a - len(df_a_new)} æ¡é‡å æ•°æ®ã€‚"
                )

                # å¦‚æœAä¸­æ‰€æœ‰æ•°æ®éƒ½é‡å ï¼Œåˆ™åˆ é™¤æ–‡ä»¶A
                if df_a_new.is_empty():
                    print(f"ğŸ—‘ï¸ æ–‡ä»¶ {file_a.name} å·²è¢«å®Œå…¨è¦†ç›–ï¼Œåˆ é™¤æ—§æ–‡ä»¶ã€‚")
                    file_a.unlink()
                else:
                    # åˆ é™¤æ—§æ–‡ä»¶A
                    file_a.unlink()
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ–‡ä»¶: {file_a.name}")

                    # ä½¿ç”¨å†™ç¼“å­˜å‡½æ•°ï¼Œå®ƒå¯ä»¥å¤„ç†æ–‡ä»¶å‘½åå’Œè·¯å¾„
                    write_to_cache(
                        symbol, period, df_a_new, cache_dir, cache_size, file_type
                    )

    print("\n--- é‡å æ£€æŸ¥å’Œæ¸…ç†å®Œæˆ ---")
