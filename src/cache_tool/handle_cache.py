import pandas as pd
import numpy as np
from pathlib import Path
import shutil
from datetime import datetime, timezone
from typing import Callable


# ==================== æ ¸å¿ƒé€»è¾‘å‡½æ•° ====================


def mock_fetch_ohlcv(
    symbol: str, period: str, start_time: int, count: int, exchange: any = None
) -> pd.DataFrame:
    """
    æ¨¡æ‹Ÿç”Ÿæˆ OHLCV æ•°æ®çš„ Pandas DataFrameã€‚
    åˆ—: time, open, high, low, close, volume
    """
    time_step_ms = _period_to_ms(period)
    data = [
        [start_time + i * time_step_ms, 100 + i, 105 + i, 98 + i, 102 + i, 1000 + i]
        for i in range(count)
    ]
    return pd.DataFrame(
        data, columns=["time", "open", "high", "low", "close", "volume"]
    )


def fetch_ohlcv(
    symbol: str, period: str, start_time: int, count: int, exchange: any = None
) -> pd.DataFrame:
    if not exchange:
        return pd.DataFrame()
    data = exchange.fetchOHLCV(symbol, period, start_time, count)
    return pd.DataFrame(
        data, columns=["time", "open", "high", "low", "close", "volume"]
    )


def _handle_cache_write(
    symbol: str,
    period: str,
    new_data: pd.DataFrame,
    cache_dir: Path,
    cache_size: int,
    file_type: str = ".parquet",
) -> None:
    """
    å¤„ç†æ–°æ•°æ®å†™å…¥ç¼“å­˜çš„é€»è¾‘ï¼ŒåŒ…æ‹¬å¤„ç†é‡å å’Œè¦†ç›–æƒ…å†µã€‚

    æ­¤å‡½æ•°é‡‡ç”¨â€œå…ˆå¤„ç†åå†™å…¥â€çš„ç­–ç•¥ï¼Œé€šè¿‡è®°å½•é‡å è¾¹ç•Œï¼Œæœ€åä¸€æ¬¡æ€§å¯¹æ–°æ•°æ®è¿›è¡Œåˆ‡ç‰‡å¹¶å†™å…¥ã€‚
    """
    if new_data.empty:
        return

    new_data_start = new_data.iloc[0, 0]
    new_data_end = new_data.iloc[-1, 0]

    sorted_cache_files = _get_sorted_cache_files(cache_dir, symbol, period)

    start_time_to_write = new_data_start
    end_time_to_write = new_data_end

    files_to_delete = []

    for f in sorted_cache_files:
        info = _get_file_info(f.name)
        if not info:
            continue
        old_data_start = info["start_time"]
        old_data_end = info["end_time"]

        # 1. æ–°æ•°æ®å®Œå…¨åœ¨æ—§æ•°æ®å†…éƒ¨ï¼ˆå®Œå…¨è¢«ç¼“å­˜ï¼‰ -> æ— éœ€å†™å…¥ï¼Œç›´æ¥è¿”å›
        if new_data_start >= old_data_start and new_data_end <= old_data_end:
            print(
                f"âœ… æ–°æ•°æ® ({new_data_start}-{new_data_end}) å·²è¢«ç¼“å­˜æ–‡ä»¶ ({old_data_start}-{old_data_end}) å®Œå…¨è¦†ç›–ï¼Œæ— éœ€å†™å…¥ã€‚"
            )
            return

        # 2. æ–°æ•°æ®å®Œå…¨è¦†ç›–æ—§æ•°æ® -> æ ‡è®°æ—§æ–‡ä»¶ä¸ºåˆ é™¤
        elif new_data_start <= old_data_start and new_data_end >= old_data_end:
            print(
                f"ğŸ”„ æ–°æ•°æ® ({new_data_start}-{new_data_end}) å®Œå…¨è¦†ç›–æ—§ç¼“å­˜æ–‡ä»¶ ({old_data_start}-{old_data_end})ï¼Œæ ‡è®°ä¸ºåˆ é™¤ã€‚"
            )
            files_to_delete.append(f)

        # 3. æ–°æ•°æ®åœ¨æ—§æ•°æ®ä¹‹å‰ä¸”æœ‰é‡å  -> è°ƒæ•´å†™å…¥çš„ç»“æŸæ—¶é—´ï¼Œä½¿å…¶ä¸åŒ…å«é‡å éƒ¨åˆ†
        elif new_data_end > old_data_start and new_data_start < old_data_start:
            end_time_to_write = min(end_time_to_write, old_data_start)
            print(
                f"âš ï¸ æ–°æ•°æ® ({new_data_start}-{new_data_end}) ä¸æ—§ç¼“å­˜ ({old_data_start}-{old_data_end}) é‡å ï¼Œè°ƒæ•´å†™å…¥ç»“æŸæ—¶é—´ã€‚"
            )

        # 4. æ–°æ•°æ®åœ¨æ—§æ•°æ®ä¹‹åä¸”æœ‰é‡å  -> è°ƒæ•´å†™å…¥çš„èµ·å§‹æ—¶é—´ï¼Œä½¿å…¶ä¸åŒ…å«é‡å éƒ¨åˆ†
        elif new_data_start < old_data_end and new_data_end > old_data_end:
            start_time_to_write = max(start_time_to_write, old_data_end)
            print(
                f"âš ï¸ æ–°æ•°æ® ({new_data_start}-{new_data_end}) ä¸æ—§ç¼“å­˜ ({old_data_start}-{old_data_end}) é‡å ï¼Œè°ƒæ•´å†™å…¥èµ·å§‹æ—¶é—´ã€‚"
            )

    # æ‰§è¡Œåˆ é™¤æ“ä½œ
    if files_to_delete:
        for f in files_to_delete:
            if f.exists():
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶: {f.name}")
                f.unlink()

    # æœ€åæ ¹æ®è°ƒæ•´åçš„æ—¶é—´èŒƒå›´è¿›è¡Œåˆ‡ç‰‡å’Œå†™å…¥
    data_to_write = new_data[
        (new_data["time"] >= start_time_to_write)
        & (new_data["time"] <= end_time_to_write)
    ]

    if not data_to_write.empty:
        _write_to_cache(symbol, period, data_to_write, cache_dir, cache_size, file_type)
    else:
        print("âŒ ç»è¿‡å¤„ç†ï¼Œæ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥ç¼“å­˜ã€‚")


def get_ohlcv_with_cache(
    symbol: str,
    period: str,
    start_time: int,
    count: int,
    cache_dir: Path,
    cache_size: int,
    page_size: int,
    enable_cache: bool = True,
    file_type: str = ".parquet",
    fetch_callback: Callable = mock_fetch_ohlcv,
    fetch_callback_params: dict = {},
) -> pd.DataFrame:
    """
    æ ¹æ®æ–°çš„ç»Ÿä¸€é€»è¾‘è·å–Kçº¿æ•°æ®ï¼Œæ”¯æŒç¼“å­˜ã€‚
    """
    cache_dir = Path(cache_dir)

    fetched_data = pd.DataFrame()
    current_time = start_time
    remaining_count = count

    while remaining_count > 0:
        # 1. ä¼˜å…ˆä»ç¼“å­˜è·å–æ•°æ®
        cached_chunk = pd.DataFrame()
        if enable_cache and current_time is not None:
            # æ‰¾åˆ°ä» current_time å¼€å§‹çš„è¿ç»­ç¼“å­˜æ•°æ®å—
            cached_chunk = _get_next_continuous_cache_chunk(
                cache_dir, symbol, period, current_time, remaining_count, file_type
            )

        if not cached_chunk.empty and not (
            len(cached_chunk) == 1
            and len(fetched_data) > 0
            and cached_chunk.iloc[-1, 0] == fetched_data.iloc[-1, 0]
        ):
            print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œå·²åŠ è½½ {len(cached_chunk)} æ¡æ•°æ®ã€‚")

            fetched_data = _merge_with_deduplication(fetched_data, cached_chunk)

            remaining_count = count - len(fetched_data)
            print(len(fetched_data), remaining_count)
            if remaining_count <= 0:
                print("âœ… ç¼“å­˜å®Œå…¨å‘½ä¸­ï¼Œç›´æ¥è¿”å›ã€‚")
                break

            current_time = fetched_data.iloc[-1, 0]
        else:
            # 2. ç¼“å­˜ä¸è¶³æˆ–æ— ç¼“å­˜ï¼Œå‘èµ·ç½‘ç»œè¯·æ±‚
            print("âš ï¸ ç¼“å­˜ä¸è¶³æˆ–æ— ç¼“å­˜ï¼Œå¼€å§‹è¯·æ±‚æ–°æ•°æ®ã€‚")
            fetch_limit = min(remaining_count, page_size)

            # ä¿®å¤æ­»å¾ªç¯é—®é¢˜ï¼šå¦‚æœå·²è·å–æ•°æ®ï¼Œä¸”æœ€åä¸€æ¬¡è¯·æ±‚çš„æ•°æ®é‡å¯èƒ½å› ä¸ºé‡å è€Œä¸è¶³ï¼Œåˆ™å¤šè¯·æ±‚ä¸€æ¡ã€‚
            if not fetched_data.empty:
                fetch_limit += 1

            new_data = fetch_callback(
                symbol, period, current_time, fetch_limit, **fetch_callback_params
            )

            if new_data.empty:
                print("âŒ æ•°æ®æºè¿”å›ç©ºï¼Œåœæ­¢è¯·æ±‚ã€‚")
                break

            # 3. å°†æ–°æ•°æ®ä¸å·²è·å–æ•°æ®åˆå¹¶ï¼Œå¹¶å¤„ç†é‡å 
            fetched_data = _merge_with_deduplication(fetched_data, new_data)

            # 4. å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå°†æ–°æ•°æ®å†™å…¥ç¼“å­˜
            if enable_cache:
                _handle_cache_write(
                    symbol, period, new_data, cache_dir, cache_size, file_type
                )

            if len(new_data) < fetch_limit:
                print("è¯·æ±‚æ•°ç»„ä¸è¶³ï¼Œåœæ­¢è¯·æ±‚ã€‚")
                break

            remaining_count = count - len(fetched_data)
            if remaining_count <= 0:
                print("âœ… æ•°æ®è¯·æ±‚å®Œæ¯•ï¼Œè¿”å›ç»“æœã€‚")
                break

            current_time = fetched_data.iloc[-1, 0] + 1000

    # è¿”å›ä¹‹å‰å…ˆæ¸…ç†ä¸åˆå¹¶ä¸€ä¸‹å°æ–‡ä»¶
    consolidate_cache(cache_dir, cache_size, symbol, period, file_type)

    # 5. è¿”å›æœ€ç»ˆæ•°æ®ï¼Œå¹¶ç¡®ä¿æ•°é‡æ­£ç¡®
    return fetched_data.iloc[:count]


def _merge_with_deduplication(
    cached_data: pd.DataFrame, chunk: pd.DataFrame
) -> pd.DataFrame:
    """
    å°†æ–°æ•°æ®ä¸ç°æœ‰æ•°æ®åˆå¹¶ï¼Œå¹¶å¤„ç†é¦–å°¾é‡å çš„æ•°æ®ç‚¹ã€‚

    Args:
        cached_data (pd.DataFrame): å·²æœ‰çš„ç¼“å­˜æ•°æ®ã€‚
        chunk (pd.DataFrame): æ–°åŠ è½½çš„æ•°æ®å—ã€‚

    Returns:
        pd.DataFrame: åˆå¹¶å¹¶å»é‡åçš„ DataFrameã€‚
    """
    if cached_data.empty:
        return chunk

    if chunk.empty:
        return cached_data

    # æ£€æŸ¥é¦–å°¾æ˜¯å¦ç›¸ç­‰
    if chunk.iloc[0, 0] == cached_data.iloc[-1, 0]:
        # ç”¨æ–°æ•°æ®çš„ç¬¬ä¸€è¡Œæ›¿æ¢æ—§æ•°æ®çš„æœ€åä¸€è¡Œ
        cached_data.iloc[-1] = chunk.iloc[0]
        # ç§»é™¤æ–°æ•°æ®çš„ç¬¬ä¸€è¡Œï¼Œå‡†å¤‡åˆå¹¶
        chunk = chunk.iloc[1:]

    # å¦‚æœæ–°æ•°æ®å—å¤„ç†åä¸ä¸ºç©ºï¼Œåˆ™è¿›è¡Œåˆå¹¶
    if not chunk.empty:
        return pd.concat([cached_data, chunk], ignore_index=True)
    else:
        return cached_data


def _get_next_continuous_cache_chunk(
    cache_dir: Path,
    symbol: str,
    period: str,
    start_time: int,
    target_count: int,
    file_type: str = ".parquet",
) -> pd.DataFrame:
    """
    å¯»æ‰¾å¹¶åŠ è½½ä»æŒ‡å®šæ—¶é—´å¼€å§‹çš„è¿ç»­ç¼“å­˜æ•°æ®å—ã€‚
    """
    cached_data = pd.DataFrame()

    sorted_files = _get_sorted_cache_files(cache_dir, symbol, period, file_type)

    start_file = None

    # éå†ä¸€æ¬¡æ‰€æœ‰æ–‡ä»¶ï¼Œæ ¹æ®ä¼˜å…ˆçº§æ‰¾åˆ°æœ€ä½³åŒ¹é…
    for f in sorted_files:
        info = _get_file_info(f.name)
        if not info:
            continue

        if info["start_time"] == start_time:
            # ä¼˜å…ˆçº§1ï¼šç²¾ç¡®åŒ¹é…ï¼Œè¯·æ±‚çš„èµ·å§‹æ—¶é—´ç­‰äºæ–‡ä»¶çš„èµ·å§‹æ—¶é—´
            start_file = f
            print("ä¼˜å…ˆçº§1")
            break
        elif info["start_time"] < start_time < info["end_time"]:
            # ä¼˜å…ˆçº§2ï¼šè¯·æ±‚çš„èµ·å§‹æ—¶é—´ä½äºæ–‡ä»¶æ—¶é—´åŒºé—´å†…
            start_file = f
            print("ä¼˜å…ˆçº§2")
            break
        elif info["start_time"] <= start_time <= info["end_time"]:
            # ä¼˜å…ˆçº§3ï¼šé»˜è®¤åŒ¹é…ï¼Œå¦‚æœå­˜åœ¨å¤šä¸ªæ—¶é—´åŒºé—´é‡å çš„æ–‡ä»¶ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„
            start_file = f
            print("ä¼˜å…ˆçº§3")
            break

    if start_file is None:
        return cached_data

    # ä»åŒ¹é…çš„æ–‡ä»¶å¼€å§‹åŠ è½½æ•°æ®
    first_chunk = _read_cache_file(start_file, file_type)

    # ç²¾ç¡®åˆ‡ç‰‡ï¼šæ‰¾åˆ° start_time å¯¹åº”çš„è¡Œå¹¶åˆ‡ç‰‡
    try:
        start_index = first_chunk.index[first_chunk["time"] == start_time][0]
        cached_data = first_chunk.iloc[start_index:]
    except IndexError:
        # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…çš„ç´¢å¼•ï¼Œåˆ™è¿”å›ç©º DataFrame
        return pd.DataFrame()

    # å°†å½“å‰æ—¶é—´æ›´æ–°ä¸ºå·²åŠ è½½æ•°æ®çš„æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œç”¨äºåç»­çš„è¿ç»­æ€§æ£€æŸ¥
    current_time = cached_data.iloc[-1, 0]

    # ç»§ç»­åŠ è½½åç»­è¿ç»­çš„ç¼“å­˜æ–‡ä»¶
    start_file_index = sorted_files.index(start_file)
    for i in range(start_file_index + 1, len(sorted_files)):
        filepath = sorted_files[i]
        info = _get_file_info(filepath.name)

        # æ£€æŸ¥æ˜¯å¦è¿ç»­ï¼šä¸‹ä¸€ä¸ªæ–‡ä»¶çš„èµ·å§‹æ—¶é—´æ˜¯å¦ç­‰äºå½“å‰å·²åŠ è½½æ•°æ®çš„ç»“æŸæ—¶é—´
        if info["start_time"] == current_time:
            chunk = _read_cache_file(filepath, file_type)

            cached_data = _merge_with_deduplication(cached_data, chunk)

            # æ›´æ–°å½“å‰æ—¶é—´ä¸ºæ–°åˆå¹¶æ•°æ®çš„æœ€åä¸€ä¸ªæ—¶é—´ç‚¹
            current_time = cached_data.iloc[-1, 0]

            if len(cached_data) >= target_count:
                # å¦‚æœå·²åŠ è½½çš„æ•°æ®é‡è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œè¿›è¡Œåˆ‡ç‰‡å¹¶åœæ­¢åŠ è½½
                cached_data = cached_data.iloc[:target_count]
                break
        else:
            # æ–‡ä»¶ä¸è¿ç»­ï¼Œåœæ­¢æŸ¥æ‰¾
            break

    return cached_data


def _read_cache_file(filepath: Path, file_type: str) -> pd.DataFrame:
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–ç¼“å­˜æ–‡ä»¶ã€‚
    """
    try:
        if file_type == ".parquet":
            return pd.read_parquet(filepath)
        elif file_type == ".csv":
            return pd.read_csv(filepath)
        else:
            raise ValueError(f"Unsupported file type for reading: {file_type}")
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {filepath.name} ({file_type}): {e}")
        return pd.DataFrame()


def _write_cache_file(filepath: Path, data: pd.DataFrame, file_type: str) -> None:
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹å†™å…¥ç¼“å­˜æ–‡ä»¶ã€‚
    """
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºçˆ¶ç›®å½•
        if file_type == ".parquet":
            print("å†™å…¥ç¼“å­˜æ–‡ä»¶", filepath)
            data.to_parquet(filepath, index=False)
        elif file_type == ".csv":
            print("å†™å…¥ç¼“å­˜æ–‡ä»¶", filepath)
            data.to_csv(filepath, index=False)
        else:
            raise ValueError(f"Unsupported file type for writing: {file_type}")
    except Exception as e:
        print(f"âŒ æ— æ³•å†™å…¥æ–‡ä»¶ {filepath.name} ({file_type}): {e}")


def _convert_ms_timestamp_to_utc_datetime(ms_timestamp: int) -> datetime:
    """
    å°†æ¯«ç§’çº§æ—¶é—´æˆ³è½¬æ¢ä¸ºå¸¦æœ‰ UTC æ—¶åŒºä¿¡æ¯çš„ datetime å¯¹è±¡ã€‚
    """
    return datetime.fromtimestamp(ms_timestamp / 1000, tz=timezone.utc)


def _format_timestamp(dt: datetime) -> str:
    """
    æ ¼å¼åŒ– datetime å¯¹è±¡ä¸º 'YYYYMMDDTHHMMSSZ' UTC æ ¼å¼ã€‚
    """
    dt = dt.astimezone(timezone.utc)
    return dt.strftime("%Y%m%dT%H%M%SZ")


def _period_to_ms(period: str) -> int:
    """
    å°† K çº¿å‘¨æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¯«ç§’ã€‚
    æ”¯æŒ '1m', '5m', '1h', '1d' ç­‰ã€‚
    """
    if period.endswith("m"):
        return int(period[:-1]) * 60 * 1000
    elif period.endswith("h"):
        return int(period[:-1]) * 3600 * 1000
    elif period.endswith("d"):
        return int(period[:-1]) * 24 * 3600 * 1000
    else:
        raise ValueError(f"Unsupported period format: {period}")


def _parse_timestamp_string(ts_str: str) -> int:
    """
    è§£æ 'YYYYMMDDTHHMMSSZ' æ ¼å¼çš„å­—ç¬¦ä¸²ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³ã€‚
    """
    dt = datetime.strptime(ts_str, "%Y%m%dT%H%M%SZ")
    return int(dt.replace(tzinfo=timezone.utc).timestamp() * 1000)


def _get_file_info(filename: str) -> dict | None:
    """è§£ææ–‡ä»¶åï¼Œæå– start_time, end_time, count"""
    try:
        parts = Path(filename).stem.split(" ")
        symbol, period, start_str, end_str, count = (
            parts[0],
            parts[1],
            parts[2],
            parts[3],
            int(parts[4]),
        )
        return {
            "symbol": symbol,
            "period": period,
            "count": count,
            "start_time": _parse_timestamp_string(start_str),
            "end_time": _parse_timestamp_string(end_str),
        }
    except (ValueError, IndexError):
        return None


def sanitize_symbol(symbol: str) -> str:
    """
    æ¸…ç†äº¤æ˜“å¯¹ç¬¦å·ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä¾‹å¦‚å°† 'BTC/USDT' è½¬æ¢ä¸º 'BTC_USDT'ã€‚
    """
    # æ„å»ºä¸€ä¸ªè½¬æ¢è¡¨ï¼Œå°† '/' å’Œ ':' æ˜ å°„ä¸º '_'
    # str.maketrans() åˆ›å»ºäº†ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå°†å­—ç¬¦ ':', '/' æ˜ å°„åˆ° '_'
    translator = str.maketrans("/:", "__")
    return symbol.translate(translator)


def _get_sorted_cache_files(
    cache_dir: Path, symbol: str, period: str, file_type: str = ".parquet"
) -> list[Path]:
    """
    æ ¹æ® symbol å’Œ period è·å–å¹¶æ’åºç¼“å­˜ç›®å½•ä¸‹ç‰¹å®š symbol å’Œ period çš„æ–‡ä»¶åã€‚
    """
    if not cache_dir.exists():
        return []

    all_files = [f for f in cache_dir.iterdir() if f.suffix == file_type]

    valid_and_matched_files = [
        f
        for f in all_files
        if (info := _get_file_info(f.name))
        and info["symbol"] == sanitize_symbol(symbol)
        and info["period"] == period
    ]

    return sorted(
        valid_and_matched_files, key=lambda f: _get_file_info(f.name)["start_time"]
    )


def _write_to_cache(
    symbol: str,
    period: str,
    data: pd.DataFrame,
    cache_dir: Path,
    cache_size: int,
    file_type: str = ".parquet",
) -> None:
    """
    å°†æ•°æ®å†™å…¥ç¼“å­˜ï¼Œå¹¶æ ¹æ® cache_size åˆ†å‰²æˆå¤šä¸ªæ–‡ä»¶ã€‚

    Args:
        symbol (str): äº¤æ˜“å¯¹ã€‚
        period (str): Kçº¿å‘¨æœŸã€‚
        data (pd.DataFrame): å¾…å†™å…¥çš„ OHLCV æ•°æ®ã€‚
        cache_dir (Path): ç¼“å­˜ç›®å½•è·¯å¾„ã€‚
        cache_size (int): æ¯ä¸ªç¼“å­˜æ–‡ä»¶å­˜å‚¨çš„æ•°æ®è¡Œæ•°ã€‚
    """
    if data.empty:
        return

    if not cache_dir.exists():
        cache_dir.mkdir(parents=True)

    start_index = 0
    total_rows = len(data)

    while start_index < total_rows:
        end_index = min(start_index + cache_size, total_rows)
        chunk = data.iloc[start_index:end_index]

        if chunk.empty:
            break

        chunk_start_ts = chunk.iloc[0, 0]
        chunk_end_ts = chunk.iloc[-1, 0]
        chunk_count = len(chunk)

        chunk_start_dt = _convert_ms_timestamp_to_utc_datetime(chunk_start_ts)
        chunk_end_dt = _convert_ms_timestamp_to_utc_datetime(chunk_end_ts)
        chunk_start_str = _format_timestamp(chunk_start_dt)
        chunk_end_str = _format_timestamp(chunk_end_dt)

        filename = f"{sanitize_symbol(symbol)} {period} {chunk_start_str} {chunk_end_str} {chunk_count:04d}{file_type}"
        filepath = cache_dir / filename

        _write_cache_file(filepath, chunk, file_type)
        print(f"  > å†™å…¥ç¼“å­˜æ–‡ä»¶: {filename}")

        start_index += cache_size


def consolidate_cache(
    cache_dir: Path,
    cache_size: int,
    symbol: str,
    period: str,
    file_type: str = ".parquet",
) -> None:
    """
    æ•´ç†ç¼“å­˜ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚

    æŸ¥æ‰¾è¿ç»­çš„ã€å¤§å°å°äº cache_size çš„ç¼“å­˜æ–‡ä»¶ï¼Œå°†å®ƒä»¬åˆ†ç»„ï¼Œç„¶ååˆå¹¶åé‡æ–°å†™å…¥ã€‚
    """
    if not cache_dir.exists():
        print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ•´ç†ã€‚")
        return

    sorted_cache_files = _get_sorted_cache_files(cache_dir, symbol, period, file_type)
    if not sorted_cache_files:
        print("âœ… ç¼“å­˜ç›®å½•ä¸­æ²¡æœ‰éœ€è¦æ•´ç†çš„æœ‰æ•ˆæ–‡ä»¶ã€‚")
        return

    # ä½¿ç”¨äºŒç»´åˆ—è¡¨æ¥æ”¶é›†è¿ç»­çš„æ–‡ä»¶å—
    files_to_merge = []

    print(f"\n--- å¼€å§‹åˆ†ç»„ {symbol} {period} çš„ç¼“å­˜æ–‡ä»¶ ---")

    current_group = []
    for i in range(len(sorted_cache_files)):
        current_file = sorted_cache_files[i]
        current_info = _get_file_info(current_file.name)
        if not current_info:
            continue

        # åªæœ‰å½“æ–‡ä»¶å¤§å°å°äº cache_size æ—¶æ‰è€ƒè™‘å°†å…¶åŠ å…¥å¾…åˆå¹¶é˜Ÿåˆ—
        if current_info["count"] < cache_size:
            # æ£€æŸ¥æ˜¯å¦ä¸å‰ä¸€ä¸ªæ–‡ä»¶è¿ç»­
            is_continuous = False
            if current_group:
                last_file_info = _get_file_info(current_group[-1].name)
                # æ£€æŸ¥å½“å‰æ–‡ä»¶çš„å¼€å§‹æ—¶é—´æ˜¯å¦ä¸å‰ä¸€ä¸ªæ–‡ä»¶çš„ç»“æŸæ—¶é—´ç›¸ç­‰
                if (
                    last_file_info
                    and current_info["start_time"] == last_file_info["end_time"]
                ):
                    is_continuous = True

            if not current_group or is_continuous:
                current_group.append(current_file)
                print(f"âœ… å°†æ–‡ä»¶ {current_file.name} æ·»åŠ åˆ°å½“å‰è¿ç»­ç»„ã€‚")
            else:
                # é‡åˆ°ä¸è¿ç»­çš„æ–‡ä»¶ï¼Œä¿å­˜å½“å‰ç»„å¹¶å¼€å§‹æ–°çš„ç»„
                files_to_merge.append(current_group)
                current_group = [current_file]
                print(f"âš ï¸ é‡åˆ°ä¸è¿ç»­ï¼Œæ–°å¼€ä¸€ç»„ï¼Œæ·»åŠ æ–‡ä»¶ {current_file.name}ã€‚")
        else:
            # é‡åˆ°å¤§äºæˆ–ç­‰äº cache_size çš„æ–‡ä»¶ï¼Œç»“æŸå½“å‰ç»„
            if current_group:
                files_to_merge.append(current_group)
                current_group = []

    # å¾ªç¯ç»“æŸæ—¶ï¼Œä¿å­˜æœ€åä¸€ä¸ªç»„
    if current_group:
        files_to_merge.append(current_group)

    print("\n--- å¼€å§‹åˆå¹¶å’Œé‡æ–°å†™å…¥ç¼“å­˜æ–‡ä»¶ ---")
    merged_count = 0

    for group in files_to_merge:
        # åªæœ‰å½“ä¸€ä¸ªç»„åŒ…å«å¤šäºä¸€ä¸ªæ–‡ä»¶æ—¶æ‰è¿›è¡Œåˆå¹¶
        if len(group) > 1:
            merged_count += 1
            print(f"--- æ­£åœ¨å¤„ç†ç¬¬ {merged_count} ä¸ªå¾…åˆå¹¶æ–‡ä»¶å— ---")

            merged_data = pd.DataFrame()

            # 1. åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ–‡ä»¶
            for f in group:
                try:
                    data_to_merge = _read_cache_file(f, file_type)
                    merged_data = _merge_with_deduplication(merged_data, data_to_merge)
                    print(f"ğŸ“¦ å·²åŠ è½½å¹¶åˆå¹¶æ–‡ä»¶: {f.name}")
                except Exception as e:
                    print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {f.name}: {e}")

            # 2. åˆ é™¤æ—§æ–‡ä»¶
            for f in group:
                if f.exists():
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶: {f.name}")
                    f.unlink()

            # 3. å†™å…¥æ–°åˆå¹¶çš„æ•°æ®
            _write_to_cache(
                symbol, period, merged_data, cache_dir, cache_size, file_type
            )

    if merged_count == 0:
        print("âœ… æ²¡æœ‰éœ€è¦åˆå¹¶çš„æ–‡ä»¶å—ï¼Œç¼“å­˜å·²æ˜¯æœ€ä¼˜çŠ¶æ€ã€‚")

    print("--- ç¼“å­˜æ•´ç†å®Œæˆ ---")
