import pandas as pd
from pathlib import Path
from typing import Callable

from .cache_fetcher import mock_fetch_ohlcv, fetch_ohlcv
from .cache_core import (
    handle_cache_write,
)
from .cache_read_chunk import get_next_continuous_cache_chunk
from .cache_data_processor import merge_with_deduplication

from .cache_consolidator import consolidate_cache


def get_ohlcv_with_cache(
    symbol: str,
    period: str,
    start_time: int,
    count: int,
    cache_dir: Path,
    cache_size: int,
    page_size: int,
    enable_cache: bool = True,
    file_type: str = "parquet",
    fetch_callback: Callable = mock_fetch_ohlcv,
    fetch_callback_params: dict = {},
) -> pd.DataFrame:
    """
    æ ¹æ®æ–°çš„ç»Ÿä¸€é€»è¾‘è·å–Kçº¿æ•°æ®ï¼Œæ”¯æŒç¼“å­˜ã€‚
    """
    cache_dir = Path(cache_dir)

    fetched_data = pd.DataFrame()
    current_time = start_time
    remaining_count = count

    while remaining_count > 0:
        print(
            f"\nğŸŸ¡ ç›®æ ‡æ•°æ®é‡: {count}ï¼Œå·²è·å–: {len(fetched_data)}ï¼Œå¾…è·å–: {remaining_count}"
        )

        # 1. ä¼˜å…ˆä»ç¼“å­˜è·å–æ•°æ®
        cached_chunk = pd.DataFrame()
        if enable_cache and current_time is not None:
            # æ‰¾åˆ°ä» current_time å¼€å§‹çš„è¿ç»­ç¼“å­˜æ•°æ®å—
            cached_chunk = get_next_continuous_cache_chunk(
                cache_dir, symbol, period, current_time, remaining_count, file_type
            )

        if not cached_chunk.empty and not (
            len(cached_chunk) == 1
            and len(fetched_data) > 0
            and cached_chunk.iloc[-1, 0] == fetched_data.iloc[-1, 0]
        ):
            print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œå·²åŠ è½½ {len(cached_chunk)} æ¡æ•°æ®ã€‚")

            fetched_data = merge_with_deduplication(fetched_data, cached_chunk)

            remaining_count = count - len(fetched_data)
            print(len(fetched_data), remaining_count)
            if remaining_count <= 0:
                print("âœ… ç¼“å­˜å®Œå…¨å‘½ä¸­ï¼Œç›´æ¥è¿”å›ã€‚")
                break

            current_time = fetched_data.iloc[-1, 0]
        else:
            # 2. ç¼“å­˜ä¸è¶³æˆ–æ— ç¼“å­˜ï¼Œå‘èµ·ç½‘ç»œè¯·æ±‚
            print("â„¹ï¸ ç¼“å­˜ä¸è¶³æˆ–æ— ç¼“å­˜ï¼Œå¼€å§‹è¯·æ±‚æ–°æ•°æ®ã€‚")
            fetch_limit = min(remaining_count, page_size)

            # # ä¿®å¤æ­»å¾ªç¯é—®é¢˜ï¼šå¦‚æœå·²è·å–æ•°æ®ï¼Œä¸”æœ€åä¸€æ¬¡è¯·æ±‚çš„æ•°æ®é‡å¯èƒ½å› ä¸ºé‡å è€Œä¸è¶³ï¼Œåˆ™å¤šè¯·æ±‚ä¸€æ¡ã€‚
            if not fetched_data.empty and fetch_limit < page_size:
                fetch_limit += 1
                print("é¢„é˜²æ­»å¾ªç¯", fetch_limit)

            new_data = fetch_callback(
                symbol, period, current_time, fetch_limit, **fetch_callback_params
            )
            print(f"fetchæ•°æ® {len(new_data)}")

            if new_data.empty:
                print("âŒ æ•°æ®æºè¿”å›ç©ºï¼Œåœæ­¢è¯·æ±‚ã€‚")
                break

            # 3. å°†æ–°æ•°æ®ä¸å·²è·å–æ•°æ®åˆå¹¶ï¼Œå¹¶å¤„ç†é‡å 
            fetched_data = merge_with_deduplication(fetched_data, new_data)

            # 4. å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå°†æ–°æ•°æ®å†™å…¥ç¼“å­˜
            if enable_cache:
                handle_cache_write(
                    symbol, period, new_data, cache_dir, cache_size, file_type
                )

            if len(new_data) < fetch_limit:
                print("è¯·æ±‚æ•°ç»„ä¸è¶³ï¼Œåœæ­¢è¯·æ±‚ã€‚")
                break

            remaining_count = count - len(fetched_data)
            if remaining_count <= 0:
                print("âœ… æ•°æ®è¯·æ±‚å®Œæ¯•ï¼Œè¿”å›ç»“æœã€‚")
                break

            current_time = fetched_data.iloc[-1, 0]

    # è¿”å›ä¹‹å‰å…ˆæ¸…ç†ä¸åˆå¹¶ä¸€ä¸‹å°æ–‡ä»¶
    consolidate_cache(cache_dir, cache_size, symbol, period, file_type)

    # 5. è¿”å›æœ€ç»ˆæ•°æ®ï¼Œå¹¶ç¡®ä¿æ•°é‡æ­£ç¡®
    return fetched_data.iloc[:count]
