import polars as pl
from pathlib import Path
from typing import Callable

from .cache_fetcher import mock_fetch_ohlcv, fetch_ohlcv
from .cache_write_overlap_handler import handle_cache_write
from .cache_read_chunk import get_next_continuous_cache_chunk
from .cache_data_processor import merge_with_deduplication

from .cache_consolidator import consolidate_cache, check_for_overlaps
from .cache_utils import (
    get_chunk_slices,
    format_timestamp,
    convert_ms_timestamp_to_utc_datetime,
)
from filelock import Timeout, FileLock


def get_ohlcv_with_cache_lock(*args, **kargs):
    cache_dir = kargs.get("cache_dir") or (args[4] if len(args) > 4 else None)

    lock = FileLock(f"{cache_dir}/cache.lock")
    with lock:
        return get_ohlcv_with_cache(*args, **kargs)


def get_ohlcv_with_cache(
    symbol: str,
    period: str,
    start_time: int | None,
    count: int,
    cache_dir: Path,
    cache_size: int,
    page_size: int,
    enable_cache: bool = True,
    file_type: str = "parquet",
    fetch_callback: Callable = mock_fetch_ohlcv,
    fetch_callback_params: dict = {},
    enable_consolidate: bool = True,
) -> pl.DataFrame:
    """
    æ ¹æ®æ–°çš„ç»Ÿä¸€é€»è¾‘è·å–Kçº¿æ•°æ®ï¼Œæ”¯æŒç¼“å­˜ã€‚
    start_time: å¦‚æœä¸ºNone, ä¸ä¼šè¯»å–ç¼“å­˜, ä½†æ˜¯ä¼šå†™å…¥ç¼“å­˜=

    """
    cache_dir = Path(cache_dir)
    file_type = file_type = file_type.lstrip(".")

    fetched_data = pl.DataFrame()
    current_time = start_time

    print(
        "start_time",
        start_time,
        "count",
        count,
        "page_size",
        page_size,
        "cache_size",
        cache_size,
    )

    if count == 0:
        print("â„¹ï¸ countä¸º0, è¿”å›ç©ºdataframe")
        return fetched_data

    if start_time is None:
        print(f"â„¹ï¸ start_timeä¸ºNone, å¼€å§‹è¯·æ±‚æ–°æ•°æ®ã€‚{count}")

        new_data = fetch_callback(
            symbol, period, current_time, count, **fetch_callback_params
        )

        if enable_cache:
            print("â„¹ï¸ start_timeä¸ºNone, åªå†™å…¥ç¼“å­˜, ä¸è¯»å–ç¼“å­˜")
            handle_cache_write(
                symbol, period, new_data, cache_dir, cache_size, file_type
            )
        return new_data

    chunk_slices = get_chunk_slices(count, page_size)

    for slice in chunk_slices:
        start_slice, end_slice = slice
        current_count = end_slice - start_slice
        need_count = current_count

        print(
            f"\nğŸŸ¡ ç›®æ ‡æ•°æ®é‡: {count}ï¼Œå·²è·å–: {len(fetched_data)} pagesç´¢å¼•: {start_slice}-{end_slice}"
        )

        # 1. ä¼˜å…ˆä»ç¼“å­˜è·å–æ•°æ®
        cached_chunk = pl.DataFrame()
        if enable_cache and current_time is not None:
            # æ‰¾åˆ°ä» current_time å¼€å§‹çš„è¿ç»­ç¼“å­˜æ•°æ®å—
            cached_chunk = get_next_continuous_cache_chunk(
                cache_dir,
                symbol,
                period,
                current_time,
                current_count,
                file_type,
            )

        # ç¡®ä¿ current_time ä¸ä¸º None
        if current_time is None:
            # This should not happen if logic is correct, but satisfies type checker
            raise ValueError("Unexpected None for current_time")

        _current_time = format_timestamp(
            convert_ms_timestamp_to_utc_datetime(current_time)
        )

        if not cached_chunk.is_empty():
            print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œå·²åŠ è½½ {len(cached_chunk)} æ¡æ•°æ®ã€‚{_current_time}")
            fetched_data = merge_with_deduplication(fetched_data, cached_chunk)

            need_count = current_count - len(cached_chunk)
            need_count = 0 if need_count <= 0 else need_count + 1
            current_time = fetched_data.tail(1)["time"].item()

        if need_count > 0:
            print(f"â„¹ï¸ éœ€è¦ {need_count} æ¡æ•°æ®ï¼Œå¼€å§‹è¯·æ±‚æ–°æ•°æ®ã€‚{_current_time}")

            new_data = fetch_callback(
                symbol, period, current_time, need_count, **fetch_callback_params
            )

            if new_data.is_empty():
                print("âŒ æ•°æ®æºè¿”å›ç©ºï¼Œæå‰åœæ­¢è¯·æ±‚ã€‚")
                break

            fetched_data = merge_with_deduplication(fetched_data, new_data)
            current_time = fetched_data.tail(1)["time"].item()

            # 4. å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå°†æ–°æ•°æ®å†™å…¥ç¼“å­˜
            if enable_cache:
                handle_cache_write(
                    symbol, period, new_data, cache_dir, cache_size, file_type
                )

            if len(new_data) < need_count:
                print("âŒ è¯·æ±‚æ•°ç»„ä¸è¶³ï¼Œæå‰åœæ­¢è¯·æ±‚ã€‚")
                break

    if enable_consolidate:
        check_for_overlaps(cache_dir, cache_size, symbol, period, file_type)
        consolidate_cache(cache_dir, cache_size, symbol, period, file_type, start_time)

    return fetched_data
