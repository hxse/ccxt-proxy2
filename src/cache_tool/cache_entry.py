import pandas as pd
from pathlib import Path
from typing import Callable

from .cache_fetcher import mock_fetch_ohlcv, fetch_ohlcv
from .cache_write_overlap_handler import handle_cache_write
from .cache_read_chunk import get_next_continuous_cache_chunk
from .cache_data_processor import merge_with_deduplication

from .cache_consolidator import consolidate_cache, check_for_overlaps
from .cache_utils import (
    get_chunk_slices,
    format_timestamp,
    convert_ms_timestamp_to_utc_datetime,
)


def get_ohlcv_with_cache(
    symbol: str,
    period: str,
    start_time: int,
    count: int,
    cache_dir: Path,
    cache_size: int,
    page_size: int,
    enable_cache: bool = True,
    file_type: str = "parquet",
    fetch_callback: Callable = mock_fetch_ohlcv,
    fetch_callback_params: dict = {},
    enable_consolidate: bool = True,
) -> pd.DataFrame:
    """
    æ ¹æ®æ–°çš„ç»Ÿä¸€é€»è¾‘è·å–Kçº¿æ•°æ®ï¼Œæ”¯æŒç¼“å­˜ã€‚
    start_time: å¦‚æœä¸ºNone, ä¸ä¼šè¯»å–ç¼“å­˜, ä½†æ˜¯ä¼šå†™å…¥ç¼“å­˜=

    """
    cache_dir = Path(cache_dir)
    file_type = file_type = file_type.lstrip(".")

    fetched_data = pd.DataFrame()
    current_time = start_time

    if count == 0:
        return fetched_data

    print(
        "start_time",
        start_time,
        "count",
        count,
        "page_size",
        page_size,
        "cache_size",
        cache_size,
    )

    chunk_slices = get_chunk_slices(count, page_size)

    for slice in chunk_slices:
        start_slice, end_slice = slice
        current_count = end_slice - start_slice
        need_count = current_count

        print(
            f"\nğŸŸ¡ ç›®æ ‡æ•°æ®é‡: {count}ï¼Œå·²è·å–: {len(fetched_data)} pagesç´¢å¼•: {start_slice}-{end_slice}"
        )

        # 1. ä¼˜å…ˆä»ç¼“å­˜è·å–æ•°æ®
        cached_chunk = pd.DataFrame()
        if enable_cache and current_time is not None:
            # æ‰¾åˆ°ä» current_time å¼€å§‹çš„è¿ç»­ç¼“å­˜æ•°æ®å—
            cached_chunk = get_next_continuous_cache_chunk(
                cache_dir,
                symbol,
                period,
                current_time,
                current_count,
                file_type,
            )

        _current_time = format_timestamp(
            convert_ms_timestamp_to_utc_datetime(current_time)
        )

        if not cached_chunk.empty:
            print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼Œå·²åŠ è½½ {len(cached_chunk)} æ¡æ•°æ®ã€‚{_current_time}")
            fetched_data = merge_with_deduplication(fetched_data, cached_chunk)

            need_count = current_count - len(cached_chunk)
            need_count = 0 if need_count <= 0 else need_count + 1
            current_time = fetched_data.iloc[-1, 0]

        if need_count > 0:
            print(f"â„¹ï¸ éœ€è¦ {need_count} æ¡æ•°æ®ï¼Œå¼€å§‹è¯·æ±‚æ–°æ•°æ®ã€‚{_current_time}")

            new_data = fetch_callback(
                symbol, period, current_time, need_count, **fetch_callback_params
            )

            if new_data.empty:
                print("âŒ æ•°æ®æºè¿”å›ç©ºï¼Œæå‰åœæ­¢è¯·æ±‚ã€‚")
                break

            fetched_data = merge_with_deduplication(fetched_data, new_data)
            current_time = fetched_data.iloc[-1, 0]

            # 4. å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå°†æ–°æ•°æ®å†™å…¥ç¼“å­˜
            if enable_cache:
                handle_cache_write(
                    symbol, period, new_data, cache_dir, cache_size, file_type
                )

            if len(new_data) < need_count:
                print("âŒ è¯·æ±‚æ•°ç»„ä¸è¶³ï¼Œæå‰åœæ­¢è¯·æ±‚ã€‚")
                break

    if enable_consolidate:
        check_for_overlaps(cache_dir, cache_size, symbol, period, file_type)
        consolidate_cache(cache_dir, cache_size, symbol, period, file_type, start_time)

    return fetched_data
